[{"categories":["investing"],"content":"Berkshire Hathaway held it’s first virtual AGM this weekend, thanks to the ongoing pandemic. Buffett first spoke for an hour about some history of the market, followed by the formal shareholder’s meeting, and roughly 2 hours of Q\u0026A to top it off. His presentation was a masterclass in market history, time travel and public speaking. All questions this year had to be routed through the journalist panel of Becky Quick, Carol Loomis and Andrew Ross Sorkin. The resulting questions were of really high quality. I quite liked this virtual-only format. Here are my high level takeaways from the event. Future of Berkshire: It’s quite clear to me that Greg Abel is the new CEO. He has stressed in the past that the CEO of Berkshire needs to be a capital allocator and was caught a little offguard when he was asked why he didn’t mention Ajit amongst the capital allocators. I found Greg Abel to be confident, but his answers (obviously) lacked the sharp insights we have come to expect of Warren \u0026 Charlie. Perhaps, this will resolve itself with time. Technology: As Buffett said, the top 5 companies aren’t very expensive. They are very special and return gobs of cash while not needing much capital at all. Reference to the Great Depression: His first hour of presentation, though splendid in content, was somber in tone. He later on has repeatedly said that 20-30 years is a good time frame (up from his 10 that he said and sold equity index puts on in the recession). Betting on America: Buffett is no different in his optimism on America. Charlie generally tones his enthusiasm down. In his examples, his start date for 5000:1 real wealth generation are from a very special point – newfoundland with nothing but opportunity. New Land of this size at these latitudes hadn’t been found for at least half a millennia before and probably never will be found going forward (Mars?). The wealth of minerals under the land also was a bounty. In short, this was a very special case that won’t repeat. The next 20 years are going to be interesting. This, IMO, will be due to a combination of rising inequality, inability to accept higher wealth redistribution, loss of sole superpower status, and overall climate of liberal malaise. Charlie said it beautifully during DJCO - he said that, despite the various challenges, Japan handled the last 20 years with grace. Lack of Buybacks: Lack of buybacks and share purchases was very surprising. Buffett’s hand is effectively telling us that he thinks we are in for a rough time ahead or that we don’t know. He quoted Mr. Market just a few and farmland analogy minutes before the buyback question. On the other hand, if book at $155 is reasonable, $185 isn’t too expensive. Change: Every individual faces change as they age and the environment evolves around them. I think Warren is changing too - he’s moving from a growth mindset to a capital preservation mindset over the last few years. Further, the capital he manages is becoming impossible to deploy if he rules out 4 out of the top 5 companies. My key takeaways are twofold. First, technology businesses with very high rates of capital return can continue driving the US equity market. Second, given starting valuations in the rest of the world (Europe, SE Asia), they may be a better bet for the non-tech businesses. Potentially, other than overweighting these two factors a bit over the coming years, I’m not making major changes to my investment plan. ","date":"2020-04-06","objectID":"/posts/2020-05-05-berkshire-2020/:0:0","tags":null,"title":"Berkshire 2020: Notes from the Virtual AGM","uri":"/posts/2020-05-05-berkshire-2020/"},{"categories":["investing"],"content":"After nearly a decade, the much awaited bear market is on us. It didn’t arrive in any form or shape that anyone would have liked or predicted, but that’s how regimes change in the equity markets. I plan to write up my thoughts soon, but in the meantime, I wanted to share this excellent podcast interview of JL Collins on the Mad Fientist. Link to Podcast: here. My notes from this podcast: Every bear market is unique. By definition, every time there’s a bear market, it feels different. If it turns out to be the end of civilization, we will have more things to worry about than our investments. If you didn’t have uncertainty or a scary thing you would not have a bear market. It’s the definition. Corona virus won’t be as bad as the black plague. We understand a lot more about hygeine and germ theory than they did in 1300s. Imagine you own a hotel by the beach and there’s a giant hurricane coming. You will perhaps temporarily close the hotel, maybe you are forced to lay off some employees, but you’re not going to sell it just because the outlook over the next few months is negative. We are compensated with higher returns in stocks exactly because of times like this. If the stock market was always smooth sailing, then we shouldn’t get paid much more than safe investments like bonds should give us. Going back to the hotel example, If you want to own a business like that you need to be prepared to withstand the occasional hurricane. If you’re not prepared, you will suffer. The time to prepare is not in the middle of the hurricane but it is when there is calm on the horizon. If you’re going to invest in stocks, you need to understand that bear markets are part of the landscape. You shouldn’t be any more surprised by them then you should be by blizzards in northern New England. Based on the comments JL Collins is getting on his blog, he estimates that about 2/3 of his commenters have understood this need to prepare. They are sleeping well. The remaining 1/3 of his readers are panicking and he feels that they did not understand themselves and their risk tolerances. In 1987, on Black Monday, the market fell over 23%. It still remains the biggest in history. While Collins was invested, he didn’t realize it fell until a casual evening conversation with his broker that he initiated purely by chance. The flow of news from then to today is just worlds apart. After that market drop in 1987, the market continue to grind down slowly over time. Despite knowing that he should stay the course, JL Collins couldn’t control his nerve and got out close to the bottom. The market then began its relentless right up and he kept watching, it kept expecting things to fall and of course that never happened. He finally got back in much later at higher price. That lesson is what sustained Jim Collins in 2007-2008. The host of the podcast, Brandon, had some excess cash in 2007 due to sale of his house. He kept buying but the market kept tanking and he would buy less and less each time. He could never fully deploy his cash. As a result, he now really believes in the power of systems. If you are in the wealth accumulation phase, and are spending less than you earn, a bear market like this is a huge gift. Such people should keep investing as aggressively as they can. If you are in the wealth preservation stage, that’s the time to introduce bonds in the portfolio – in order to smooth the ride at times like this by rebalancing annually. Ideally you would have done self reflection during calm and figured out how much in stocks works for you. But if you haven’t this is a good time to learn about your behavior. The bear markets almost always happen rapidly. There’s a saying on Wall Street, “stocks take the stairs up and take the elevator down”. You cannot predict the bottom. The last two days of last week (2020 March 9-13) are a perfect example of that. The market had its worst day since 1987 followed by one of its best days right after. Anyone who thinks they know what’s","date":"2020-03-16","objectID":"/posts/2020-03-16-bear-market-notes-jl-collins/:0:0","tags":null,"title":"Notes from JL Collins on the market crash (Mad Fientist podcast)","uri":"/posts/2020-03-16-bear-market-notes-jl-collins/"},{"categories":["investing"],"content":"“You and Your Research” is one of my favorite lectures on the scientific process. In it, Turing Award winner, Richard Hamming, speaks about the importance of selecting the right problem. This key idea turns out to be of massive significance in the investing process as well. Consider the last 5 years for US and Emerging equity markets: As widely known, US markets far outperformed emerging equity markets. However, this is only part of the story. Of greater interest is the outcome of actual investors in these markets. While we can’t actually study individual investor results, we can look at the distribution of mutual fund returns in each space. Using morningstar’s fund categorization, there are 1091 Largecap US Equity, and 576 Emerging market equity funds that existed over the last 5 years. Running a percentile analysis on these funds looks as follows: US Large Equity Funds (n = 1091) DescriptionPercentile5 Year Cumulative Returns Worst 5 29.06% Median5054.78% Best 9568.20% Emerging Market Funds (n = 576) DescriptionPercentile5 Year Cumulative Returns Worst 5 -3.27% Median5018.71% Best 9534.31% As expected the median Emerging equity fund did worse than the median US equity fund. However, that’s just the tip of the iceberg. Here’s the real news headline: Even the best emerging equity funds performed worse than the worst US equity funds. Let that sink in for a bit. There are many takeaways here: The pond we fish in matters more than how smart or hard we fish within the pond. The go-anywhere investor truly does have an advantage in the modern world. The relative amount of outperformance in US equity markets is lower than the relative amount of underperformance. This is probably due to fees and expenses. Too often in investing we concern ourselves with performance numbers and results without taking into context the environment in which they were achieved. Performance only ever makes sense in context. But perhaps the most important question is the following: Given that markets are mean reverting over the long run, how will the next 5 years be for these groups? ","date":"2019-04-15","objectID":"/posts/2019-04-15-fishing-where-the-fish-are/:0:0","tags":null,"title":"Fishing where the fish are","uri":"/posts/2019-04-15-fishing-where-the-fish-are/"},{"categories":["investing"],"content":"Sometimes, we are aware that something is very likely to happen, but we don’t fully digest the implications until we go through the motions. For many years now, John C. Bogle, the founder of Vanguard group has signalled about his declining health. Earlier today, news arrived that he passed away. John Bogle was a selfless legend of the investment world. By creating one of the first index funds at very low fees, he provided the world with an elegant way to get market returns. Incidentally, this arose out of his undergraduate thesis at Princeton, where he studied the performance of mutual funds and concluded that the vast majority of them underperform the market. Every mutual fund is owned by it’s shareholders. John Bogle’s second insight was to make the fund company (Vanguard) mutually owned by the funds it managed. This removed the need for any profit. All profits are distributed back to shareholders. In doing so, he chose the path not travelled. Many of his competitors such as Mr. Charles Schwab and the Johnsons (of Fidelity) are billionaires. They became rich by managing money for others and earning fees on the investments, even if the investors themselves didn’t do very well for themselves. As John Bogle has said: The investors came for the high fees, and stayed for the underperformance. Three decades ago, when Vanguard was a fledgling in the investment world, it was common for mutual funds to charge 1%+ yearly fees. In addition, they would also charge 12b-1 sales commissions of 5%+. In other words, if you invested $100, only $95 would end up in the fund (after sales commissions) and you would pay 1% annually! By giving investors low fee access to market return, Vanguard started the race to the bottom. Today, average index fund fees are around 0.1% (and many times even lower). More than $3 trillion are invested in index funds today. Thanks to Vanguard, the world has saved over 0.9% per year in many trillions of assets. The amount that Mr. Bogle has saved the world is in the trillions of dollars on its own. When he started the index fund, Mr. Bogle was ridiculed, and even called, “un-American”, for buying every stock in the market. Nevertheless, he stuck with it and launched the fund. It was undersubscribed and raised only $11M. The fund didn’t even have enough money to buy every stock in the S\u0026P 500. To solve this, they sampled randomly from the S\u0026P500. It worked just enough to allow the fund to live and continue. It really is due to his grit and perseverance that we have low-cost index funds today! John C Bogle was a selfless legend, and an independent thinker. He is one of my investing and life heroes, and it is incredibly sad to see him pass away. His contributions will outlive all of us for many generations to come. ","date":"2019-01-16","objectID":"/posts/2019-01-16-selfless-legend-bogle/:0:0","tags":null,"title":"The selfless legend: John Bogle","uri":"/posts/2019-01-16-selfless-legend-bogle/"},{"categories":["investing"],"content":"Post on Reversion to the Mean. Link to John Bogle’s post and chart. https://docs.google.com/spreadsheets/d/e/2PACX-1vTwliubDGhJtRLXlq0Pj3kajY2bJSk6zrmJaMqK4Yl79UxqVPgS6Evrw9yf0mMLHk-50AWa4nnlxt7C/pubchart?oid=572780189\u0026format=image ","date":"2018-07-18","objectID":"/posts/2018-07-02-rtm/:0:0","tags":null,"title":"Reversion to the Mean","uri":"/posts/2018-07-02-rtm/"},{"categories":["investing"],"content":"With low fees, excellent diversification, and probabilities in the investor’s favor, indexing and passive investing have been on the rise for the last 40 years. Vanguard is a leader in this field, offering investors access to the world’s stocks for less than a tenth of a percent. A common question that repeatedly arises is “What if everybody indexes?”. While I don’t think this is an issue (since prices are set at the margin and more that 95% of trading on the exchanges is the result of active flows) this question is not without merit. To even begin answering this question, it is important to understand the current size of passive indexing. This is not as obvious as it may initially seem. FT reports that passive investing is a third of the market, another source says it’s 24%, while another estimate says it’s less than 10%. So, which is it? In order to answer this, a ground up approach is required. The key is that most passive investing happens at the large fund houses, and these are listed in 13-D and 13-F filings at the SEC. One can also look at an aggregator like Morningstar for some of this data. To estimate this, I looked at three large stocks, and made a best effort to classify the largest institutional holders as active vs passive. I chose GE, Apple, and Microsoft in order to avoid dealing with significant float adjustments. As an example, here is the table for GE along with my best effort classification of passive vs active nature of the institutional holder. InstitutionWeightPassive Vanguard Group Inc7.00Yes State Street Corp3.85Yes GE Savings and Security Program4.29No BlackRock Institutional Trust Company NA2.64Yes Capital World Investors1.81No Capital Research Global Investors1.49No Northern Trust Investments N A1.34No Fidelity Management and Research Company1.07Yes Harris Associates L.P.1.01No Geode Capital Management, LLC0.99No Government Pension Fund of Norway - Global0.7No State Street Global Advisors (Aus) Ltd1.28Yes Trian Fund Management LP0.82No Franklin Advisers Inc0.79Yes T. Rowe Price Associates, Inc.0.68Yes Managed Account Advisors LLC0.68No Merrill Lynch \u0026 Co Inc0.59No Wellington Management Company LLP0.59No Morgan Stanley Smith Barney LLC0.56No Barrow Hanley Mewhinney \u0026 Strauss LLC0.75No Adding only the passive ownership, we get the following: Average19.4% Apple20.25% Microsoft20.75% GE17.31% However, we must not forget that there are several passive funds that aren’t on the list above. We can adjust for this by using the pareto rule. We also need to adjust for the fact that all passive focused institutions including Vanguard themselves have several active funds. In fact, Vanguard itself has around 20% of it’s AUM in active funds ($1T / $5T). Average19.4% Adjust for Tail (Pareto rule)24.30% Passive Multiplier0.7 Passive Indexing17% Thus, the estimate for passive indexing is ~17%. This is not a large number by any means and I suspect that it will ebb and flow with the markets itself. Passive indexing is efficient in many ways, including costs, diversification, and portfolio risk, but on it’s own, it does not remove the behavioral fragilities of the individual investor! ","date":"2018-06-20","objectID":"/posts/2018-06-20-passive-indexing/:0:0","tags":null,"title":"An estimate of the size of passive indexing","uri":"/posts/2018-06-20-passive-indexing/"},{"categories":["investing"],"content":"Founded by Oaktree Capital in 2014, STORE Capital holds a diversified real estate portfolio across the US. Berkshire Hathaway bought 9.8% of the company last summer via a new equity issuance. I recently read the 2017 Annual Report of the company and found it a great learning experience. Click here to read more. ","date":"2018-03-18","objectID":"/posts/2018-03-19-stor/:0:0","tags":null,"title":"STORE Capital","uri":"/posts/2018-03-19-stor/"},{"categories":["investing"],"content":"A few months ago, I wrote about Fairfax Financial Holdings. Fairfax India Holdings(FIH) is a partly-owned subsidiary of Fairfax Financial Holdings that primarily invests in India. FIH IPOed in 2014 at $10 per share. After a rise of nearly 70%, recent pullbacks put FIH back at around book value. To read more, click here. ","date":"2017-12-21","objectID":"/posts/2017-12-21-fih/:0:0","tags":null,"title":"Fairfax India Holdings","uri":"/posts/2017-12-21-fih/"},{"categories":["investing"],"content":"Recently, there have been many pieces that claim that value investing has not worked this cycle. Einhorn questioned if value investing is dead and Goldman Sachs mulled it. Others have suggested that value works, but it has evolved. The claim is that quantitative measures no longer work (certainly not P/B, at least) and identifying value is about qualitative analysis. For example, growing companies with wide moats and ability to reinvest capital are regularly mispriced. All of this appeals to intuition. Several value managers have underperformed and some others have closed shop. But is it true? To find out, I ran a few backtests. Quite immediately, it was clear that P/B has not worked as well as it used to. This is a well known fact, and the diminishing nature of this factor on large cap stocks is referenced in What Works On Wall Street (James O’ Shaughnessy). In the same book, the author suggests using a Value Composite. Such a methodology ranks all stocks on P/B, P/E, and P/FCF, and uses the sum of these ranks to decide the final ranking. The book itself was written in the 90s (and updated hence), so any strategy that we pick from the original version is hindsight-bias free. The Value Composite has worked wonders in the last 10 years. The backtests I ran showed 10-year CAGR of ~8% and a 5-year CAGR of 15%, heftily beating the index. To make sure that my data was survivorship bias free, and my code was error free, I investigated other ways of verifying the effectiveness of the value anamoly. I didn’t have to look far, since the MSCI’s enhanced value index (pdf) uses a methodology that is very similar in spirit: Rank stocks based on the z-scores of P/B, Forward P/E, and EV/CFO Average these scores for each stock. Within a sector, scale and cap these scores to make it comparable. Build a value-cum-market weighted index of the top ~100 securities. This simple, quantitative method, holding a diversified portfolio, beats the market cap weighted index: Now, obviously an index is gross of fees, and cannot be invested in directly. However, the MSCI Enhanced Value Index has ETFs tracking it. VLUE in particular, is a low cost (~15 bps) ETF offered by iShares that tracks the benchmark fairly closely. In short, even quantitative value investing on a diversified portfolio has performed quite well. Value investing is dead, long live value investing (link). ","date":"2017-11-26","objectID":"/posts/2017-11-26-value-in-the-cycle/:0:0","tags":null,"title":"Value investing during the last bull cycle","uri":"/posts/2017-11-26-value-in-the-cycle/"},{"categories":["investing"],"content":"Fairfax financial holdings was formed when Prem Watsa and a motley crew of four others took over Markel Financial Canda in 1985. Since then, Fairfax has compounded it’s book at nearly 20% per year. A $10,000 investment in 1985 would be worth nearly $2 million today. Many consider Fairfax to be the Berkshire Hathaway of Canada. However, the last few years have not been easy for the company due to increased caution exercised by the management. After some changes to the investment portfolio last year, and unlocking hidden value in the last two quarters, the future looks promising. To read more click here. ","date":"2017-09-14","objectID":"/posts/2017-09-14-ffh/:0:0","tags":null,"title":"Fairfax Financial Holdings","uri":"/posts/2017-09-14-ffh/"},{"categories":["investing"],"content":"S\u0026P Global Inc is a collection of four incredible businesses. It consists of the Ratings business, commodity intelligence, markets intelligence, and the S\u0026P indices. Each one of these segments is a familiar name in the financial industry. S\u0026P makes money whenever a company raises debt, when indices want to track debt ratings, when oil is traded, through analytics \u0026 data for funds, and every time an S\u0026P 500 index is bought! To read more click here. ","date":"2017-07-27","objectID":"/posts/2017-07-27-spgi/:0:0","tags":null,"title":"S\u0026P Global Inc (SPGI)","uri":"/posts/2017-07-27-spgi/"},{"categories":["psychology","investing"],"content":"I recently read, “The Professional” by Subroto Bagchi (amazon), when I came across a wonderful chapter on the value of the human touch. In it, Bagchi says that most modern professionals have forgotten the value of the human touch. He makes an extremely good case citing how most people are driven by the need to belong and appreciated more than anything else. The highlight of the chapter for me was a section on Dr. Devi Shetty. Dr. Devi Shetty runs Narayana Hrudalaya (now called Narayana Health), a heart hospital in India. Headquartered in Bengaluru, the chain operates a chain of hospitals, heart centers, primary care facilities across India. It also has one of the largest telemedicine networks in the world. With that context, please read, re-read and enjoy the section below from Bagchi’s book on Dr. Shetty. ","date":"2017-06-04","objectID":"/posts/2017-04-06-narayana-hrudalaya/:0:0","tags":null,"title":"A note on the Human Touch: Narayana Hrudayala","uri":"/posts/2017-04-06-narayana-hrudalaya/"},{"categories":["psychology","investing"],"content":"A paragraph on the human touch A professional I have deep admiration for is the internationally renowned cardiac surgeon Dr. Devi Shetty. When you visit his famous hospital, Narayana Hrudayalaya in Bangalore, you first encounter is a small shrine in front of the portico. It is a small, four-cornered structure. One side is a temple, the other a church, the third side is a mosque and the fourth a gurdwara (a place of worship for Sikhs). The doctors at Narayana Hrudayalaya will tell you this shrine is where all healing begins – from faith and then on through the hands of the doctor. Beyond the portico, as you step inside the building, is a well-organized reception area, which has a sign in Bengali that says “BENGALI SPOKEN HERE.” In Bangalore, almost everyone speaks either English or Hindi or both. Why does the hospital require a separate area within reception for patients who are more comfortable speaking Bengali? Because demographic analysis of patients has shown that most heart patients who come to the hospital are from West Bengal in India and neighboring Bangladesh and are often escorted by a fellow villager, spouse or family member for whom an alien city, a hospital and the burden of an unknown language can be daunting. Dr. Sherry has created his institution based on existential knowledge, on Understanding how a patient who visits his hospital feels. At the end of a long day, Dr. Shetty sits in his ofice and sees outpatients. He always tries to say a few words to each patient in their mother tongue. Sometimes his accent is clumsy, but his gesture instantly endears him to them and makes them feel at ease. He uses his stethoscope to examine them, something that is probably meaningless in a clinical sense because he has their entire case history, including CT scans, in front of him. But he does it because most patients from rural areas feel a doctor has not paid attention to them or examined them thoroughly unless he has listened to their heartbeat with a stethoscope. And, invariably, he will touch them at least once during the conversation. When I asked him why he did that, he told me that these days doctors do not understand the power of the human touch. There can be no healing Without touch. After he has examined the patient, a completely unhurried Dr. Shetty ends the consultation by asking, “Do you want to ask me anything?” For a man strapped for time this can be risky, because patients may engage him in a long conversation. But for Dr. Shetty this is the most important thing to do. Dr. Shetty relates to the world of medicine at the existential level. And this is what will be required of every world-class professional of tomorrow-be it a dress designer, product manager at Procter \u0026 Gamble, a physician, a family attorney or an air traffic controller. ","date":"2017-06-04","objectID":"/posts/2017-04-06-narayana-hrudalaya/:0:1","tags":null,"title":"A note on the Human Touch: Narayana Hrudayala","uri":"/posts/2017-04-06-narayana-hrudalaya/"},{"categories":["investing"],"content":"Founded in 1945, U-Haul and it’s white and orange trucks have become synonymous with do-it-yourself moving in North America. The company recently crossed 20,000 company operated stores and franchised stores across North America, leading to a growing moat in the one-way moving segment. Along with it’s growing moving segment, the company has increased it’s EPS at a CAGR of 17% over the last 10 years! Courtsey alleganu-stor.rumspeed.net Is this good business at a discount to intrinsic value? What is a reasonable projection of the future? To find out, you can read my report on U-Haul here. ","date":"2017-03-25","objectID":"/posts/2017-03-25-uhaul/:0:0","tags":null,"title":"Amerco Holdings (U-Haul)","uri":"/posts/2017-03-25-uhaul/"},{"categories":["investing"],"content":"Fast moving consumer goods (FMCGs) are an essential part of our lives today. Not all FMCGs are commodities however. Some brands continue to engage a loyal and growing user base due to superior products, global outreach, and decades of advertising. Colgate is synonymous for the entire dental care industry, as Gillette is for men’s hygiene. A billion people are being added to the middle class over the next decade. Purchasing power continues to grow and demographics continue to be favourable. We have more single person households than ever before. At the same time, there is disruption happening at an unprecedented scale. Whether it’s better online distribution like the Dollar Shave Club or a growing number of white label products, moats that were considered unbreachable have started to chip away. I surveyed Colgate, P\u0026G, and Unilever in mid January. It involved reading dozens of 10-Ks, and close to two weeks of work. At the end, I was rewarded with an increased clarity in thinking about this space. There are only two major factors that matter over a decade – organic sales growth, and capital allocation. The rest, as they say, is noise. You can find a summary of my findings here. ","date":"2017-02-04","objectID":"/posts/2017-02-04-fmcg/:0:0","tags":["personal-finance","investing"],"title":"Survey of non-food FMCGs","uri":"/posts/2017-02-04-fmcg/"},{"categories":["programming"],"content":"I’ve always used static generators to run my website. They are performant to serve, and a good one can provide both flexibility and simplicity at the same time. My first website (on my alma mater’s 20MB hosting space for every student) used a bunch of perl scripts to shove templates and content together. A couple of years ago, when I was forced to move off my department’s hosting, I stumbled upon Jekyll. ","date":"2016-12-25","objectID":"/posts/2016-12-25-hugo/:0:0","tags":["software"],"title":"From Jekyll to Hugo","uri":"/posts/2016-12-25-hugo/"},{"categories":["programming"],"content":"Why Hugo? Jekyll is quite fantastic with flexibility, but the setup leaves a lot to be desired. One needs to have the right ruby gems, the correct python packages, and if you use a theme like minimal mistakes, you’ll also need Grundle, Rake, and LESS. In order to avoid managing multiple setups, I only ever set it up in one place and writing posts had to be done by SSHing into a box. I know that there are ways to automate and streamline the setup, but I did not have the time to learn one more piece of indirection and layering. The downside to this was two fold: Writing is already a difficult task to begin with, and having to SSH into a box meant even more activation energy. I was very dependent on one box. Earlier this year, I found out about hugo, another static generator, but written in Go. It has one enormous advantage - the binary is statically linked with no dependencies. This meant that running hugo was as simple as downloading the latest release. No setups. No machine dependecies! Written in Go, Hugo was also supposed to be super fast. Of course, all of this came at the cost of the kind of hyper flexibility that Jekyll offered. Still, I was immediately interested. ","date":"2016-12-25","objectID":"/posts/2016-12-25-hugo/:0:1","tags":["software"],"title":"From Jekyll to Hugo","uri":"/posts/2016-12-25-hugo/"},{"categories":["programming"],"content":"Challenges (or what Jekyll does really well) There were three challenges with moving to Hugo: The minimal mistakes theme had not been ported. I would have to either find an alternate theme or port the theme myself. Jekyll automatically figures out related content by using a cosine similarity metric between post word vectors. Hugo does not support this and it’s still an open discussion. Hugo does not allow categories to be part of the permalink. In Jekyll, my posts would have URLs like /category/post-name. I would break URLs going forward. Soultion: The theme After browsing a lot of themes and not finding anything suitable, I decided to port the Minimal Mistakes theme to hugo. Minimal Mistakes is a huge theme, and I only ported the parts I use. I started off by downloading the raw HTML page from my Jekyll site, and then working backwards, replacing just the content and variables I needed. I created partial templates for the header, navbar, banner, author bio and the footer, and used these to write the following templates: index.html: for the home page with recent posts. _default/single.html: for about, projects and latticework) section/post.html: for the archive post/single.html: for individual blog posts such as this one. The latest version of Hugo (0.18) treats everything as a PAGE. This helped a lot in my migration, since I did not have to special case frontmatter metadata for non post pages. I am quite satisfied with the final look. I plan to open source my port of the theme after adding an example site. Solution: Related posts A solution was to patch the hugo source to add cosine similarity and support related content. But this would mean customising the binary, and losing the main benefit of the move to Hugo. I eventually settled with using tags and categories to find related content. The code to do this ended up relatively simple: {{ if index .Params \"showrelated\" | default true -}} \u003csmall\u003e\u003cb\u003eYou might also enjoy\u003c/b\u003e\u003c/small\u003e \u003cul\u003e {{ $page_link := .Permalink -}} {{ $categories := .Params.categories -}} {{ $tags := .Params.tags -}} {{ range .Site.Pages -}} {{ $has_common_tags := intersect $tags .Params.tags | len | lt 0 -}} {{ $has_common_cats := intersect $categories .Params.categories | len | lt 0 -}} {{ $page := . -}} {{ if (and (or $has_common_tags $has_common_cats) (ne $page_link $page.Permalink)) -}} \u003cli\u003e\u003ca href=\"{{ $page.Permalink }}\"\u003e{{ $page.Title }}\u003c/a\u003e\u003c/li\u003e {{ end -}} {{ end -}} \u003c/ul\u003e \u003chr /\u003e {{ end -}} If there was no other post with overlapping categories or posts, no related posts would be rendered, but the \u003cul\u003e tag would still be shown. To fix this edge case, I added a post level param showrelated that I could set to false to hide the whole section. Solution: Keeping popular URLs While Hugo does not allow for category in the permalink, it does allow for post aliases. Using this was fairly straightforward: ---- title: \"...\" aliases: - /old-category/old-name/ ---- This enabled me to keep around links of my most popular posts (example). ","date":"2016-12-25","objectID":"/posts/2016-12-25-hugo/:0:2","tags":["software"],"title":"From Jekyll to Hugo","uri":"/posts/2016-12-25-hugo/"},{"categories":["programming"],"content":"Summary Moving the site to Hugo took me the better part of 2 days, with the majority of the time going into porting the theme. It’s now easier to deploy, and much easier to write. Time will tell about the robustness of this setup. Currently, I Hugo! ","date":"2016-12-25","objectID":"/posts/2016-12-25-hugo/:0:3","tags":["software"],"title":"From Jekyll to Hugo","uri":"/posts/2016-12-25-hugo/"},{"categories":["investing"],"content":"The Story Founded in 1950, headquartered in Dublin, California, ROSS Dress For Less is the largest off price retailer in the United States1. ROSS owns 1200+ stores in 33 states of US with over 300+ stores in California alone. Ross has limited or no presence in the Upper East (NYC, New England, NJ), Alaska and Midwest. ROST’s dd’s DISCOUNTS operates 150 stores in 15 states. In 2014, ROST employed approximately 71,000 employees (non-union) including full time and part-time employees2. Retail clothing is a brutal business. It’s competitive, seasonal, and too dependent on the whims and fancies of the general public. The only positive is that, currently, it remains a tough place for the online giants to make inroads into. The way a dress “Fit\"s is qualitative and varies from brand to brand. ROST is somehow able to pull of a magical story despite these difficulties - ROST had over $11B in Total Sales in FY 2014 and (~7%) year on year sales growth. Further, as seen below, ROST has had incredibly ROIC over the last 5 years. The questions with ROST are simple: Is there really a moat going forward? Or is the ROIC a reflection of the lucky past? If so, what would be a fair price to pay for this business? ","date":"2016-03-18","objectID":"/posts/2016-03-18-analysis-rost/:1:0","tags":null,"title":"Analysis: ROSS Dress For Less (ROST)","uri":"/posts/2016-03-18-analysis-rost/"},{"categories":["investing"],"content":"Key metrics FY 2014 FY 2013 FY 2012 FY 2011 FY 2010 ROA 19.66% 21.49% 21.81% 19.91% 17.80% ROE 40.57% 41.71% 44.53% 44.02% 41.63% ROIC 31.00% 31.29% 37.67% 28.65% 29.02% EPS $2.21 $1.94 $1.77 $1.43 $1.16 FCF Per Share $4.83 $3.64 $3.15 $0.88 $0.99 Sales Growth 7.93% 5.24% 12.93% 9.44% 9.49% EPS Growth 14.02% 9.87% 23.59% 23.51% 30.65% FCF Growth 32.59% 15.62% 258.92% -11.24% -32.23% ","date":"2016-03-18","objectID":"/posts/2016-03-18-analysis-rost/:2:0","tags":null,"title":"Analysis: ROSS Dress For Less (ROST)","uri":"/posts/2016-03-18-analysis-rost/"},{"categories":["investing"],"content":"The Qualitative Factors Purchasing ROST believes that the key to its competitive advantage is to buy out of season clothing at a deep discount via professional negotiators purchasing in bulk. In their own words: We believe that our ability to effectively execute certain off-price buying strategies is a key factor in our success … By purchasing later in the merchandise buying cycle than department, specialty, and discount stores, we are able to take advantage of imbalances between retailers’ demand for products and manufacturers’ supply of those products. They look at themselves as an opportunistic purchaser, who specialize in retailing two ways: Close-outs: These are manufacturer overruns that are still in-season, but ROSS can get these at a discount because of cancelled orders. Packaway Purchases: Off-season purchases made at a deep discount that will be stored in their warehouses until a later date (sometimes even till the beginning of the same selling season the following year). Packways in particular, are perceived by the management to be a good way of increasing prestige and brand image by including national well-known brands at extremely competitive savings. ","date":"2016-03-18","objectID":"/posts/2016-03-18-analysis-rost/:3:0","tags":null,"title":"Analysis: ROSS Dress For Less (ROST)","uri":"/posts/2016-03-18-analysis-rost/"},{"categories":["investing"],"content":"Distribution Key to the above is ROST’s ability to warehouse packaways and distribute close-outs efficiently. In management’s words: We ship all of our merchandise to our stores through these distribution centers, which are large, highly automated, and built to suit our specific off-price business model. Currently ROST owns and operates five distribution processing facilities - 2 in California, 1 in Pennsylvania, and 2 in South Carolina. An additional distribution center in Shafter, California is currently under construction and expected to open in 2015 ","date":"2016-03-18","objectID":"/posts/2016-03-18-analysis-rost/:4:0","tags":null,"title":"Analysis: ROSS Dress For Less (ROST)","uri":"/posts/2016-03-18-analysis-rost/"},{"categories":["investing"],"content":"Risk Factors Retailing is a highly competitive industry. Apart from that ROSS believes it is subject to the following risk factors: Changes in level of consumer spending on apparel or home-related merchandise. Impacts from macro-economic environment affecting disposable income. A natural disaster in California - Corporate HQ, 2 operating centers, 2 warehouses, and 25% of stores in California (they carry some earthquake insurance). Ability to achieve planned gross margins by effectively managing inventories, markdowns, and inventory shortage. The risk that packaways will go out of fashion the following year. ","date":"2016-03-18","objectID":"/posts/2016-03-18-analysis-rost/:5:0","tags":null,"title":"Analysis: ROSS Dress For Less (ROST)","uri":"/posts/2016-03-18-analysis-rost/"},{"categories":["investing"],"content":"Properties All stores (except 3) are leased on a 5y - 21y (including renewals) original lease term. No store accounted for more than 1% of sales. They own all distribution centers (6), half their warehouses (4/7), and most of their office space (2/3). The leased warehouses are often ~50% the size they own, and the leased office space in LA is by far the smallest (~20% of their next biggest) office. ","date":"2016-03-18","objectID":"/posts/2016-03-18-analysis-rost/:6:0","tags":null,"title":"Analysis: ROSS Dress For Less (ROST)","uri":"/posts/2016-03-18-analysis-rost/"},{"categories":["investing"],"content":"Other Factors Margins Gross margin has consistently been between 28 to 29%. SG\u0026A has consistently accounted for just 14.5 - 15.5% for sales. Management appears to have a tight leash on cost. Earnings net of interest and taxes are consistently between 7.5-8.5% of sales over the last 5 years. Points of Note ROST has bought back shares aggressively: ~25% less shares outstanding in 5 years. ROST grew at a great clip (~12%) even during the recession which confirms their appeal as a discount brand. Concern: Efficiency - While number of stores has only increased 29% from 2010 (1055 to 1362), number of employees have increased ~44% (from ~50k to ~71k). ","date":"2016-03-18","objectID":"/posts/2016-03-18-analysis-rost/:7:0","tags":null,"title":"Analysis: ROSS Dress For Less (ROST)","uri":"/posts/2016-03-18-analysis-rost/"},{"categories":["investing"],"content":"Valuation A raw 10x FCF valuation gives us a post-split price of ~$50. (no growth taken in account) Assuming ROST’s profits grow at ~7% for the next 5 years, tethering off to inflation after that for the next 10, we arrive at an intrinsic valuation of ~$45 (4% discount rate). The current price of $57 implies an EPS growth rate of ~10%. ROST clearly has a moat, but one dependent on process. Process driven moats tend to be the weakest type of moats, as there is inherently nothing preventing competitors from copying it over a period of time. In this regard, it seems unclear how durable ROST’s competitive advantage really is. Of course, this can still be a great investment if the purchase price can compensate for these unknowns. In my opinion, the current implied growth of ~10% does not provide a sufficient margin of safety to make this call. TJ Maxx \u0026 Marshalls (owned by TJX) are larger if put together. ↩︎ This does not include temporary employees hired during the peak seasons. ↩︎ ","date":"2016-03-18","objectID":"/posts/2016-03-18-analysis-rost/:8:0","tags":null,"title":"Analysis: ROSS Dress For Less (ROST)","uri":"/posts/2016-03-18-analysis-rost/"},{"categories":["investing"],"content":"In his 1938 text, The Theory of Investment Value, John Burr Williams made an extraordinary claim: he said that the intrinsic value of a business was simply the sum of of the future cash flows of the business discounted to the present. This idea, called Discounted Cash Flow (DCF), has become the foundation of modern valuation. Most texts on DCF are written for a financial audience. This post attempts to introduce DCF to a reader from an engineering background via an IPython notebook. In doing so, we shall analyse one of the most fantastic businesses of our time, Visa Inc. You can find the rendered IPython notebook here. ","date":"2016-03-10","objectID":"/posts/2016-03-10-intro-to-dcf-visa/:0:0","tags":null,"title":"Discounted Cash Flow For Engineers: Visa Inc","uri":"/posts/2016-03-10-intro-to-dcf-visa/"},{"categories":["finance"],"content":"In this post, we look at long-term inflation adjust returns across six different asset classes - cash, residential housing, gold, industrial commodities, bonds, and stocks (equities). By long-term, we mean at least a century or more. In doing so, we shall find that equity ownership in cash flow producing businesses via stocks outperform all other categories. All charts in this post deal with real returns - i.e, returns adjusted for inflation. ","date":"2015-10-22","objectID":"/posts/2015-10-22-why-equities/:0:0","tags":["personal-finance","investing"],"title":"Why equities?","uri":"/posts/2015-10-22-why-equities/"},{"categories":["finance"],"content":"Purchasing Power of the Dollar (Cash) Most people know that cash loses value over time in proportion to inflation. The loss in value compounds over time - even conservatively low inflation rates like 2% compound to over 80% loss of value in a century! How has reality been? Far worse! The following chart shows the purchasing power of a $100 bill between 1900-2010. Remember this chart is adjusted for real returns i.e, if you had tucked away $100 in 1900, you could only buy the equivalent of $3.48 (in 1900 money) at the turn of the century. In other words, the $100 bill lost just over 96% of it’s value! ","date":"2015-10-22","objectID":"/posts/2015-10-22-why-equities/:0:1","tags":["personal-finance","investing"],"title":"Why equities?","uri":"/posts/2015-10-22-why-equities/"},{"categories":["finance"],"content":"Inflation Adjusted Housing Returns Investment in real estate via residential housing (as opposed to REITs) provokes opinions like none other. It’s also a very unique investment in a number of ways: For one, you are born short housing. Gains (and losses) in real estate are often amplified due to the leverage that a mortgage provides. Housing is far more illiquid than any of the other asset classes we consider. Investing in housing has high closing and maintenance costs involved. I highly recommend reading gocurrycracker’s “How I made $102k in real estate”. Diversification in housing is hard unless one is extremely wealthy. The data shows that residential housing in the US has maintained its value over the 110 years as the following inflation adjusted chart shows. However, even assuming a generous 2010 value of 120, the compounded annual rate of return over the last century has been just over 0.1% annually! ￼ These numbers are market averages. There have definitely been neighbourhoods and cities that have increased ten-fold in value over the last century. Market characteristics are very localized, and it is therefore very much possible to make shrewd excellent return investments with local expert knowledge. Before you get excited, a couple of fantastic resources I recommend in thinking about the costs involved are: jcollinsnh’s post on renting vs owning The NYTimes buy vs rent calculator. ","date":"2015-10-22","objectID":"/posts/2015-10-22-why-equities/:0:2","tags":["personal-finance","investing"],"title":"Why equities?","uri":"/posts/2015-10-22-why-equities/"},{"categories":["finance"],"content":"Real Gold prices per ounce over the last 7 centuries Gold has long been perceived as an inflation hedge. The chart below shows the inflation adjusted price per ounce of gold (in £)1 over the last 7 centuries. ￼ The data does show that gold has held its value - albeit over long periods of time. If you bought some gold in the early 15th century, you would have had to wait over 4 centuries for it to regain value. Gold’s biggest value add to any portfolio arguably stems from being a hedge against economic and global instability. It’s biggest issue is that it is not a cash producing asset. As always, Buffet says it best. ","date":"2015-10-22","objectID":"/posts/2015-10-22-why-equities/:0:3","tags":["personal-finance","investing"],"title":"Why equities?","uri":"/posts/2015-10-22-why-equities/"},{"categories":["finance"],"content":"A study in industrial commodities - the real price of Silver It is a well known thesis that industrial commodities tend to lose value over long periods of time due to two reasons: Technology improves the throughput per unit of industrial commodities, i.e, less units of silver are required per use case over time. The technology to mine industrial commodities improves with time. The following chart shows the real price of silver from 1344-2004 - a negative real return of return! ￼ ","date":"2015-10-22","objectID":"/posts/2015-10-22-why-equities/:0:4","tags":["personal-finance","investing"],"title":"Why equities?","uri":"/posts/2015-10-22-why-equities/"},{"categories":["finance"],"content":"Equities and Bonds over the last 100 years US equities and bonds have done astoundingly well over the last century. Adjusted for inflation, bonds have delivered a 1.6% per year, and equities have delivered a whooping 6.7% per year. An investment of $100 in US equities in 1900 would have grown to a sum equalling $71,000 in 1900 money2! Of course, the US was one of the best performing equity markets in the 20th century! What do the returns look like in other countries - for example, those more significantly affected by the world wars? Dimson, Marsh and Stanton have collected, cleaned and normalized real equity returns across the world as part of their ground breaking “Equity Risk Premia around the world”: Every country in the world had positive real equity returns - even those with negative bond returns! ","date":"2015-10-22","objectID":"/posts/2015-10-22-why-equities/:0:5","tags":["personal-finance","investing"],"title":"Why equities?","uri":"/posts/2015-10-22-why-equities/"},{"categories":["finance"],"content":"A note on real real returns Thornburg asset management has released a wonderful paper titled “A study of real real returns” (pdf). The paper studies returns across various asset clases after inflation, expenses and taxes over the last 30 years. It’s no surprise that equities come out on top - not only are equities taxed favourably, they also have some of the lowest expense ratios around: ￼ ","date":"2015-10-22","objectID":"/posts/2015-10-22-why-equities/:0:6","tags":["personal-finance","investing"],"title":"Why equities?","uri":"/posts/2015-10-22-why-equities/"},{"categories":["finance"],"content":"Summary Equities, while volatile in the short term, have provided the highest real returns over sustained periods of time. Investment in equities is ownership in a business and is hence inherently risky. It is understandable that equity owners want to be compensated for their risk, and over time, they have been handsomely rewarded! It is also easier than ever before to be invested in US and global equity markets. Vanguard provides some extremely low cost funds and ETFs. As examples: The Vanguard VOO ETF gives you exposure to the entire S\u0026P 500 at just 0.05% fees per year. Vanguard’s VXUS ETF provides exposure to over 6000 non-US large cap stocks for just 0.14% a year. A list of all of Vanguard’s ETFs is available here. Measuring the price per ounce of gold against the dollar is very tricky due to the dollar being fixed to the price of gold until 1971. ↩︎ The purchasing power of $71,000 in the 1900 is equal to $2 million dollars in 2000. ↩︎ ","date":"2015-10-22","objectID":"/posts/2015-10-22-why-equities/:0:7","tags":["personal-finance","investing"],"title":"Why equities?","uri":"/posts/2015-10-22-why-equities/"},{"categories":["finance"],"content":"What is the rule of 72 The well known rule of ++72++ estimates the time it takes for an investment to double with compounding. Simply put it states, that if an investment has a return of ++r++% year on year, the investment will double in roughly ++\\frac{72}{r}++ years. Combined with basic knowledge of compounding, the rule is both simple and useful enough to perform a number of wide variety of approximations without having to use a calculator. Let us run through two illustrative examples: Example 1 Q) At a 6% rate of return, how much money will $100,000 grow to in 30 years? The rule of 72 tells us that a return of 6% compounded should double in about 12 years. Hence, it should grow about 4x in 24 years, making it a sum of $400,000 in 4 years. The remaining 6 years is (luckily) half the doubling time. We can approximate the effect of these 6 years again using the inverse rule of 72. Assuming 6 years to be a time period we know that the investment doubles in two time periods (12 years), i.e, ++2 = \\frac{72}{r}++. This gives us a growth of 36% in one time period (6 years) - note that the result of the application of this rule of 72 is the same as simple interest of 6% for 6 years and is hence a bad one. We will come back to this later, and learn about when the rule of 72 does badly. Thus the sum of $400,000 will grow by another 36% in the last 6 years, giving us a value of $544,000 after the full 30 years. Example 2 Q) What will $100 be worth in 50 years at an inflation of 3%? Asked another way, since inflation of 3% is equal to a negative -3% return, we are looking for the present value of a future value of $100 at 3%. The rule of 72 tells us that a 3% return should double in about 24 years. Hence the investment should should grow 4x in about 48 years. In other words, the $100 dollar bill should be worth only about $25 in real terms after 48 years. We can perform a simple interest calculation for the remaining two years (ignoring compound interest). The $100 bill should be worth about 6% less than $25 or roughly $23.5.1. Approximation Errors Let us proceed to see what the actual results to both problems are. Using python, the answer to the first problem is 400e3 * (1.06)**30 or $574,000 (rounded to the nearest 1000 dollars). Similarly, the answer to the second problem is 100 / (1.03)**50 or $22.8 (rounded to the nearest ten cents). Both results are close enough to illustrate the power of 72. ","date":"2015-09-10","objectID":"/posts/2015-09-10-rule-of-72/:0:1","tags":null,"title":"The rule of 72 - why it works and when","uri":"/posts/2015-09-10-rule-of-72/"},{"categories":["finance"],"content":"Deriving the rule of 72 - Doubling under compounding The rule of compound interest tells us that a sum of ++x++ after ++t++ years compounding at a rate of ++r++ becomes: $$x * (1+\\frac{r}{100})^{t}$$ Given that the sum has doubled over ++t++ years, we are wish to find ++t++ such that: $$x *(1+\\frac{r}{100})^{t} = 2x$$ Dividing by ++x++ on both sides and writing ++\\frac{r}{100}++ as ++R++, we wish to find ++t++ such that: $$(1 + R)^{t} = 2$$ Taking the natural logarithm on both sides and transposing to make ++t++ the subject of the equation, we get: $$t = \\frac{ln 2}{ln(1+R)}$$ Let us approximate ++ln(1+R)++. Assuming ++R++ to be small, we may ignore the higher order terms in the Maclaurin series expansion of ++ln(1+R)++. This gives us ++ln(1+R) \\approx R++. For small ++R++, we may write: $$t \\approx \\frac{ln 2}{R} \\approx \\frac{0.693}{R} \\approx \\frac{0.693}{\\frac{r}{100}} \\approx \\frac{69.3}{r}$$ In other words, the rule of 72 is actually the rule of 69.3! 72 is generally chosen as an approximation to the numerator since it has a large number of factors - 2, 3, 4, 6, 8, 12, 24 and 36. I prefer to use the rule of 70 when I am dealing with an interest rate of 5% or 7% and the rule of 72 otherwise. ","date":"2015-09-10","objectID":"/posts/2015-09-10-rule-of-72/:0:2","tags":null,"title":"The rule of 72 - why it works and when","uri":"/posts/2015-09-10-rule-of-72/"},{"categories":["finance"],"content":"How well does the rule of 72 hold up? In general, approximations are only as good as the regime in which the assumptions hold - in our case, the assumption is small enough ++R++. We can compare the validity of the rules of 72 and 69.3 by comparing their results against an actual compounding calculation: Interest Rate Time to Double in Years Without approximation Rule of 69.3 Rule of 72 100% 1.00 0.69 0.72 75% 1.24 0.92 0.96 50% 1.71 1.39 1.44 25% 3.11 2.77 2.88 20% 3.80 3.47 3.60 15% 4.96 4.62 4.80 12% 6.12 5.78 6.00 10% 7.27 6.93 7.20 8% 9.01 8.66 9.00 6% 11.90 11.55 12.00 4% 17.67 17.33 18.00 3% 23.45 23.10 24.00 2% 35.00 34.65 36.00 1% 69.66 69.30 72.00 0.50%138.98 138.60 144.00 0.10%693.49 693.00 720.00 In general, the rule of 72 holds up pretty well except for the high interest rates: While the Rule of 69.3 always underestimates the actual time to double, the rule of 72 tends to overestimate the time to double for low interest rates and underestimates it otherwise: If you are curious, the rule of 72 matches the without-approximation calculation at an interest rate of around 7.85%. Incidentally, in the range of practically occuring interest rates (between 2% and 10%), the rule of 72 is more precise than the rule of 69.3! Doubling in 2 time periods Thankfully, high interest rates do not occur too frequently practice. However, a very useful special case to know is the half life of doubling. Consider an investment that is set to double in k years, it will grow by ++\\sqrt{2}++ in the first ++\\frac{k}{2}++ years, and another ++\\sqrt{2}++ in the next ++\\frac{k}{2}++ years - i.e, by 41.4% in each ++\\frac{k}{2}++ period. A similar formula holds with thirds of a doubling period and ++\\sqrt[3]{2}++. Knowledge of 41% as the doubling rate would have allowed us to better appromixate the solution to Example 1 as $400,000 * 1.41 = $564,000, a number that is even closer to the actual answer of $574,000. The astute reader will realize that we are making yet another approximation here. We ideally want ++\\frac{25.00}{1.06}++ - instead we settle with ++25.00 * (1 - 0.06)++ since ++\\frac{1}{1+x}++ roughly equals ++1 - x++ for small values of x. ↩︎ ","date":"2015-09-10","objectID":"/posts/2015-09-10-rule-of-72/:0:3","tags":null,"title":"The rule of 72 - why it works and when","uri":"/posts/2015-09-10-rule-of-72/"},{"categories":["finance"],"content":"Over the last few years, I’ve spent significant time reading Buffet’s annual letters, Security Analysis (Graham \u0026 Dodd), and listening to people like Tom Geyner (of Markel Capital) talk about their steadfast principles. I was recently asked by a friend if I could pick a 2 page summary. Without sparing it much thought, I answered in the negative. How could anything less than 31 books expound the mere basics of what makes an asset? Last weekend, I was reading through Berkshire’s 2013 letter to shareholders for the third time in the last two years, when I found a 2-page gem. In that short piece, Buffet had managed to talk about valuation, return on equity, anticipated (future) earnings, estimated growth, Mr. Market, the difference between price-and-value, and the positives of being passive and aloof. More so, without using any terminology or jargon that my friend would not understand! If you could only read 2 pages on investing, this would be it: (Reproduced from pages 17-19 of the Berkshire Hathaway 2013 Annual Chairman’s Letter) Some Thoughts About Investing “Investment is most intelligent when it is most businesslike.” – The Intelligent Investor by Benjamin Graham It is fitting to have a Ben Graham quote open this discussion because I owe so much of what I know about investing to him. I will talk more about Ben a bit later, and I will even sooner talk about common stocks. But let me first tell you about two small non-stock investments that I made long ago. Though neither changed my net worth by much, they are instructive. This tale begins in Nebraska. From 1973 to 1981, the Midwest experienced an explosion in farm prices, caused by a widespread belief that runaway inflation was coming and fueled by the lending policies of small rural banks. Then the bubble burst, bringing price declines of 50% or more that devastated both leveraged farmers and their lenders. Five times as many Iowa and Nebraska banks failed in that bubble’s aftermath than in our recent Great Recession. In 1986, I purchased a 400-acre farm, located 50 miles north of Omaha, from the FDIC. It cost me $280,000, considerably less than what a failed bank had lent against the farm a few years earlier. I knew nothing about operating a farm. But I have a son who loves farming and I learned from him both how many bushels of corn and soybeans the farm would produce and what the operating expenses would be. From these estimates, I calculated the normalized return from the farm to then be about 10%. I also thought it was likely that productivity would improve over time and that crop prices would move higher as well. Both expectations proved out. I needed no unusual knowledge or intelligence to conclude that the investment had no downside and potentially had substantial upside. There would, of course, be the occasional bad crop and prices would sometimes disappoint. But so what? There would be some unusually good years as well, and I would never be under any pressure to sell the property. Now, 28 years later, the farm has tripled its earnings and is worth five times or more what I paid. I still know nothing about farming and recently made just my second visit to the farm. In 1993, I made another small investment. Larry Silverstein, Salomon’s landlord when I was the company’s CEO, told me about a New York retail property adjacent to NYU that the Resolution Trust Corp. was selling. Again, a bubble had popped – this one involving commercial real estate – and the RTC had been created to dispose of the assets of failed savings institutions whose optimistic lending practices had fueled the folly. Here, too, the analysis was simple. As had been the case with the farm, the unleveraged current yield from the property was about 10%. But the property had been undermanaged by the RTC, and its income would increase when several vacant stores were leased. Even more important, the largest tenant’s who occupied around 20% of the project’s space – was paying rent of about $5 per foot, where","date":"2015-06-28","objectID":"/posts/2015-06-28-only-2-pages-on-investing/:0:0","tags":null,"title":"If you could only read 2 pages on investing","uri":"/posts/2015-06-28-only-2-pages-on-investing/"},{"categories":["data"],"content":"Earlier today, the New York times published a wonderful visualization on how family income affects children’s college chances. The visualization uses a new you-draw-it-first paradigm, where the reader guesses the expected relationship / trend before being shown the actual data. I found the idea very cool. Go ahead and try it, I’ll wait. It’s easy to see why this is easily one of my favourite NY Times visualizations ever. Except, they chose the wrong dataset to exhibit the you-draw-it-first paradigm. ","date":"2015-05-28","objectID":"/drafts/2015-05-28-intuitive-axes/:0:0","tags":null,"title":"Choosing intuitive axes","uri":"/drafts/2015-05-28-intuitive-axes/"},{"categories":["data"],"content":"Spoiler Alert After you proceed to draw your guess, you find out that the relationship between “Percent of children who attend college” and “Parents’ income percentile” turns out to be surprisingly1 linear. Apart from the article itself calling the result “astonishing”, the aggregate responses given by NY Times readers (as of my writing the post) looks like the shaded region in this2: As it turns out, the linearity arises only because of a few tricks at play here: Income Rank in the US is distributed ~log(Dollar Income). The original paper, Chetty, Hendren, Kline, talks about the quality of college attended as well, which the NYTimes article leaves it out. The original paper defines attending college as the mere presence of one or more 1098-T filing for a child. This definition is very broad and includes everything from universities to vocational schools. ","date":"2015-05-28","objectID":"/drafts/2015-05-28-intuitive-axes/:0:1","tags":null,"title":"Choosing intuitive axes","uri":"/drafts/2015-05-28-intuitive-axes/"},{"categories":["data"],"content":"Let’s choose an x-axis we intuitively understand Let’s really dig in and see if this is as unintuitive as the article makes it seem. First let’s collect centile income data from Raj Chetty’s Equality of Opportunity. Since raw data of college attendance and quality is not readily avaiable, let’s reconstruct them off of the graphs in the paper3. To verify our data, let’s first plot College and Attendence vs Rank as the paper does: Here’s the same chart drawn against actual Income Dollars as opposed to Income Rank on the horizontal axis: This is immediately easier to relate to as the horizontal axis is far more intuitive. We all know how much a thousand dollars is (as opposed to an income centile), and how hard it is to get a $1000 raise as opposed to how hard it is to get one that will push us up by one percentile2. ","date":"2015-05-28","objectID":"/drafts/2015-05-28-intuitive-axes/:0:2","tags":null,"title":"Choosing intuitive axes","uri":"/drafts/2015-05-28-intuitive-axes/"},{"categories":["data"],"content":"What does a percentile amount to? The article states, “Moving up a single percentile on the family-income distribution makes enrolling in college about 0.7 percentage points more likely”. That’s incredibly hard to wrap my head around. How much is a percentile in dollar terms? Again, quoting, “About $2,400 in annual income separates the bottom two dots, while nearly $1 million separates the top two”. Asking the reader to guess the relationship against Income Rank as opposed to Income Dollars is the difference between asking users to draw f(x) against log(x) as opposed to x! Using the right axes matter! ","date":"2015-05-28","objectID":"/drafts/2015-05-28-intuitive-axes/:0:3","tags":null,"title":"Choosing intuitive axes","uri":"/drafts/2015-05-28-intuitive-axes/"},{"categories":["data"],"content":"Are we measuring the right thing? College quality and attending college only tell part of the story. What’s really important is how much these children make when they are older. Chetty, Hendren, Klein do collect and present how “Family Income of Child at age of 31” varies with Parent’s Income Rank. Here’s a Google Sheets rendition of their data: Here’s the same data plotted against our preferred horizontal axis of Parent Income Dollars: This graph is far easier to understand! In fact, it begin to tell a story. A story of a world where the challenges of pursuing a quality college4 education bottoms out with income, but at the same time, a world where the money one is likely to make is directly proportional to the financial resources one has growing up. If you made it this far, here’s the data along with interactive charts. As a bonus, here’s a sweet treat for you - a Toby Morris comic that tells a similar story. The entirety of the charm of a you-draw-it-first charm presumably lies in a surprise such as this. With that considered, the surprise isn’t that surprising after all. ↩︎ Yep, my intention was to draw an S curve - but my impatience got the better of me! ↩︎ ↩︎ Extrapolation was easy since the paper mentions that college attendance is linear with rank, and college quality is quadratic. ↩︎ By college, we really mean an educational institution that had to file a Form 1098-T. ↩︎ ","date":"2015-05-28","objectID":"/drafts/2015-05-28-intuitive-axes/:0:4","tags":null,"title":"Choosing intuitive axes","uri":"/drafts/2015-05-28-intuitive-axes/"},{"categories":["psychology"],"content":"Consider renting a house in a competitive and busy area. Imagine someone who drives around many neighbourhoods looking for “To Let” signs. On finding one, he notes down the contact number, revisits it during business hours after scheduling an appointment, only to find that the house has a carpeted floor which does not suit his allergies. He repeats the process over a few weeks until he finds one suitable to his tastes and budget. Now imagine someone spending a few hours on craigslist filtering to find three houses fitting her budget and tastes. She sets up appointments with them over the phone, and has a place to herself in less than a few working days. While both our imaginary characters worked toward the same end goal, one went about it a smarter way. Now, you are thinking to yourself, “What an absurdly contrived example! No one would ever do this in real life”. I assure you it happens all the time - spending multiple minutes on a manual end-to-end sanity check each edit-compile-test cycle as opposed to first spending some time to build a unit test suite1. Storing the result of every sales call in an email as opposed to tracking coversions on a spreadsheet2. Building features straight away as opposed to first user testing them with mocks. Improving the performance of a soon to be deprecated platform / API as opposed to letting it run itself to oblivion3. Sometimes, in fact, doing nothing at all may be the best thing to do. It has never been easier to do so much with such less effort and time. Our primitive brains on the other hand, still tend to associate work done with time and energy spent. This is especially true when we have to judge work that we do not fully understand. Hearing how long it took someone to build a product often convinces us of the difficulty involved. And conversely, the better we understand the nature of work involved, the better our ability to avoid this trap. As an example, hearing how short someone’s commute is does not make us think any higher of their driving skills! As the lever of technology continues to entrench our lives, we continue to get increasingly productive with the same amount of effort using tools, methods and abstractions that an ever smaller set of people are familiar with. If we do not explicitly avoid the priming in our heads, we can very easily fall prey to judging work by the busyness of it. While this is not the right thing to do, it is often the easy thing to do. How, then, can we overcome this? Choosing the right metrics is often an answer. In the house hunting scenario, maybe we would like to measure the total time to close, or how good the deal is. In the edit-compile-test scenario, the total time to complete a task could be a good metric. Metrics are only a psuedo-indicator though. The success of certain roles (such as many engineering ones) is to prevent downstream mistakes, bugs, and “work” from arising in the first place. Only a downward trend (assuming all other things constant) can be measured, not the actual number of issues avoided or dollars saved due to prudence. In the end, familiarity with the kind of work is often the best way to judge work. Otherwise, it’s all too easy to fall into the “busy\"ness pit. If the task is infrequently performed, the cost of automation may not be the worth it. XKCD has a fantastic strip on this. ↩︎ Both of which take arguably the same time but the latter lends itself to future analysis. ↩︎ Now, don’t get me wrong. I’m not saying that doing it the latter way is always better. There are often very good reasons to doing things the harder way. For example, building a prototype first if the value of the product lies in user delight which a mock can never accomplish. Sometimes the harder way may be preferable since it can be more enjoyable. Choosing the harder way is okay as long as it is a deliberate one having weighed the pros and cons. ↩︎ ","date":"2015-04-10","objectID":"/posts/2015-04-10-smart-work/:0:0","tags":["work","behavioural economics"],"title":"The busyness of work","uri":"/posts/2015-04-10-smart-work/"},{"categories":["psychology"],"content":"Truly great things require consistent, long term effort inspired by purpose and driven by progress1. It is even more of an uphill task when micro progress may be incremental and hard to measure – how much weight have you lost since that last gym session, or perhaps how much better of an artist are you after that last painting, or even how much better is that savings account because of that coffee you made yourself this morning? Moving averages and running counters never lie. Enough gym sessions and healthy eating lead to significant weight loss. A painting a day for a year made Paris take notice of Van Gogh’s refined impressionism. A coffee made each morning saves over a thousand dollars a year (and many hours waiting in line). The decision branch at the start is often the hardest part. I’ve found consistently beginning with the smallest and most unrelated of details is enough to get the routine going. It could be the the warm coffee in my hand before I sit down for the next few hours, or perhaps putting away my cellphone at the start of a mindfulness session, or the mere the change of footwear before I head to the gym. To create a habit is to soften the blow of the decision making process by repeatedly biasing it. The truly astute reader will raise her suspicions at any mention of “truly” in fear of the no true scotsman fallacy. ↩︎ ","date":"2015-03-27","objectID":"/posts/2015-03-27-habits/:0:0","tags":["self improvement","decision making"],"title":"Habits","uri":"/posts/2015-03-27-habits/"},{"categories":["psychology"],"content":"Consider any decision that can either be taken or not taken1 and the outcome of this decision to either be good or bad2. Good OutcomeBad Outcome Decision Taken 12 Decision Not Taken 34 1 and 4 are the easy cases - much good was had, and much sadness was avoided. 2 leads to a feeling of disappointment, with much time spent on focusing on the outcome. 3 leads to regret, with much time spent on focusing on the personal choices and a desire to have done something differently. These two feelings are not the same3 and while the relative magnitude of these feelings is of course a function of personal outlook, studies4 do show that dealing with regret is harder5. When faced with hard decisions, boiling them down to dealing with disappointment (if things do not work out) versus dealing with regret (if things worked out and one had not chosen) has always given me more clarity. In the real world, decisions can also be partially taken. ↩︎ Of course, there will be a lot of subjectivity in such a labelling. What classifies and good as opposed to bad is a function of mental outlook, risk profile, and optimism. ↩︎ Zeelenberg(1998) discusses how regret leads to different future behaviour from disappointment. ↩︎ More recently, in 2009, a study by Hannah Faye, Richard Gonzalez et al showed how regret led to a greater magnitude of negative feelings and a higher probability to change a previous decision. ↩︎ There is also a geographical element to this. In The Art of Choosing, Sheena Iyengar describes how some cultures find it easier to reconcile with choices than others. Particularly autonomous cultures place the emphasis on an incorrect choice on the individual while more subservient cultures tend to be more forgiving of wrong decisions. ↩︎ ","date":"2015-02-19","objectID":"/posts/2015-02-19-the-decision-outcome-matrix/:0:0","tags":["behavioural economics","decision making","hindsight"],"title":"The decision-outcome matrix","uri":"/posts/2015-02-19-the-decision-outcome-matrix/"},{"categories":["programming"],"content":"C++11 introduces unique_ptr a wonderful way to manage the most common pointer use cases without losing sanity. While any pointer can point only to one object at a time, the unique_ptr construct further enforces only one pointer to the object at any point of time; i.e, a bijective mapping between pointers and objects. For example, std::unique_ptr\u003cFoo\u003e foo_ptr(new Foo()); is equivalent to saying that foo_ptr is the only pointer that points to the object we passed into the constructor. The dereference and member access methods are overriden so that foo_ptr looks like, sounds like and behaves like a normal pointer. One can say *foo_ptr and foo_ptr-\u003emethod() as expected. Also being consistent with denoting ownership, a unique_ptr, it cannot be copied, only moved.1 In short, a unique_ptr is cheap since it has very minimal overhead lightweight as it does not need to reference count (an object can only have one valid unique pointer at any point of time) and avoids memory leaks since it automatically frees the owned raw pointer when it goes out of scope via the destructor. Even more useful, I think, is that they constrain ownership thereby avoiding the reader to reference count2 as she reads through the code. ","date":"2014-12-30","objectID":"/posts/2014-12-30-unique_ptr/:0:0","tags":["software"],"title":"The joys of unique_ptr","uri":"/posts/2014-12-30-unique_ptr/"},{"categories":["programming"],"content":"Return values work as expected The trick to using, understanding and writing effective code with unique_ptrs is to realize that a lot of moving happens under the hood. Copying or assigning a unique pointer is disallowed, but (roughly speaking), the new C++11 move semantics automatically kick in for rvalue copys. As a result, returning unique pointers work as expected since C++ move semantics converts the rvalue copy into a move: unique_ptr\u003cFoo\u003e FooFactory(ConfigObject C) { unique_ptr\u003cFoo\u003e foo_ptr; foo_ptr.reset(new Foo()); // read config object // set desired properties on Foo return foo; } int main() { unique_ptr\u003cFoo\u003e foo_ptr = FooFactory(ConfigObject()); // Works! } ","date":"2014-12-30","objectID":"/posts/2014-12-30-unique_ptr/:0:1","tags":["software"],"title":"The joys of unique_ptr","uri":"/posts/2014-12-30-unique_ptr/"},{"categories":["programming"],"content":"Ownership is made explicit When a function accepts a pointer, questions of ownership arise. For instance, imagine a function void ColorifyFoo(Foo* uncolored_foo); Without peeking into the definition of ColorifyFoo, it is difficult to say if ColorifyFoo were to own the pointer. It gets even more difficult to predict if this were a class: class FooColorifier{ FooColorifier(Foo *uncolored_foo); // Is uncolored_foo owned? }; It is almost impossible to tell whether the FooColorifier class owns foo without looking into what the class does with it. This is where unique pointers shine. They explicitly prohibit such call sites! Trying to call a unique_pointer version of the above function: void ColorifyFoo(unique_ptr\u003cFoo\u003e uncolored_foo); in a manner like the following: unique_ptr\u003cFoo\u003e foo_ptr(new Foo()); ColorifyFoo(foo_ptr); // compiler error! would throw up a compiler error! One would have to explicitly move the pointer by calling it such: ColorifyFoo(move(foo_ptr)); // works! // now foo_ptr is NULL This makes ownership clear to the reader! Of course, if you only wanted to pass a reference to ColorifyFoo so that the pointer is not moved, you should, as you guessed right, pass a reference: void ReferenceVersionOfColorifyFoo(const Foo\u0026 uncolored_foo); // callsite like the following: unique_ptr\u003cFoo\u003e foo_ptr(new Foo()); ReferenceVersionOfColorifyFoo(*(foo_ptr)); ","date":"2014-12-30","objectID":"/posts/2014-12-30-unique_ptr/:0:2","tags":["software"],"title":"The joys of unique_ptr","uri":"/posts/2014-12-30-unique_ptr/"},{"categories":["programming"],"content":"Usage with containers Allowing containers to use pointers to their data rather than objects themselves can significantly increase the speed of move-heavy operations like sorting3. However, just using containers with raw pointers is unsafe in terms of memory. For instance, passing said containers around leads to keeping track of who owns the container to appropriately manage said memory. Due to rvalue references and move semantics, C++11 enables unique_ptr to be effortlessly used with containers. This is where they go above and beyond auto_ptr. Their usage with containers is as expected: vector\u003cunique_ptr\u003cFoo\u003e\u003e vector_of_foos; unique_ptr\u003cFoo\u003e foo_ptr = new Foo(); vector_of_foos.push_back(foo_ptr); // Throws compiler error! vector_of_foos.push_back(move(foo_ptr)); // Works! vector_of_foos.emplace_back(new Foo()); // Also works! Move-heavy operations like sort, shuffle and reverse can really benefit (sometimes even upto 10x) via a judicious use of unique_ptr. ","date":"2014-12-30","objectID":"/posts/2014-12-30-unique_ptr/:0:3","tags":["software"],"title":"The joys of unique_ptr","uri":"/posts/2014-12-30-unique_ptr/"},{"categories":["programming"],"content":"The headfake In reality, the unique_ptr is actually a very good headfake. Sure, they help manage memory, but their real benefit is for the reader4! Consistent usage of unique_ptr makes ownership extremely clear. Pointers have historically been semantically overloaded to handle both ownership as well as cheap read copies. Separating the two use cases by the usage of unique_ptr5 to serve ownership and the usage of const references to allow for cheap read copies removes the ambiguity and improves programmer-read time. Programmer-read time is, of course, the most important time constraint of all! None of this is radically new. auto_ptr was the first attempt at thinking about RIAA pointer semantics, but it did not work well with containers. C++11 fixes this with the addition of move semantics. ↩︎ I’ve often joked that readers of programs have to have execution models simultaneously operating in their heads. With programming languages that do not automatically manage memory such as C / C++, they also need to reference count pointers and references. IMO, this is the source of the increased programmer productivity that Spolsky refers to in his API War post. ↩︎ And by a smaller amount, even lookup operations like searches due to cache coherence. ↩︎ One of the most frequently recurring readers is often a future version of the author! ↩︎ And it’s sibling shared_ptr for the multiple ownership case. ↩︎ ","date":"2014-12-30","objectID":"/posts/2014-12-30-unique_ptr/:0:4","tags":["software"],"title":"The joys of unique_ptr","uri":"/posts/2014-12-30-unique_ptr/"},{"categories":["hmm"],"content":"I found to my horror earlier this week that, everyday, there are 4 hours of power cuts in Hyderabad. These are generally scheduled and occur in 2 batches of 2 hours each. This particular week seems to have had awry timings and durations via unscheduled power cuts. Anyway, why power cuts1, and why unscheduled? The old rationale seems to be to ration power to match supply. I’m not entirely convinced by this: A lot of usage is necessary and will happen at another time of the day if the power is cut. Examples include doing the daily laundry in the washing machine and ironing (pressing) clothes. An increasing number of homes are starting to have inverters and UPSes to manage these power outages. These devices tend to generally have an advertised combined storage and dispatch efficiency of around 80%. While these devices tend to power fewer appliances, their decreased efficiencies somewhat counter balance the effective power savings. The weather here is rather pleasant at this time of the year, removing the need for 24x7 air conditioners (and to an extent, geysers). In fact, it is a rather easy experiment (with controls) to design and conduct for the authorities that be at the electricity board. For a given locality, it is fairly easy to measure the net power usage for A week with unscheduled power cuts for n hours in the area. A week with scheduled power cuts for n hours in the area. A week with no power cuts at all. Chances are somebody somewhere has this data already. I would be very interested to see how much these figures differ by. If they do not differ by much, maybe we need to think about a max-min fairness based power rationing method2. It would also be interesting to see how the power saved percentages vary with n, the number of hours of power cut. I also suspect the relationship would be sub-linear till the point of average inverter capacity. There are also actual solutions out of this problem (other than people buying inverters). Here are two immediate ones I can think of: Most houses here have a 3-phase connection. Blacking out two out of three phases randomly will prevent people from continuously using high wattage devices while allowing people to have some necessary access to power. The government investing in a low power fuse at each endpoint’s meter will allow the electricity board to implement a “low power mode” for each house. Of course, either of the above systems can be gamed with some rewiring, but that’s sort of what’s happening with inverters3 anyway. Alright, it’s 2014 outside and we are still talking about 15% power outages in a metropolitan city in India. Maybe we should pretend the problem does not exist. Anybody has a carpet I can sweep this under? I’m somewhat aware of the politics between the electricity board, office spaces, and the existence of certain sums of money that will prevent power being rationed for certain parties. I’m choosing to ignore the will-cut-power-to-bully argument and focus on the possibility of any scientific reasons. ↩︎ There must be a joke in here somewhere involving sine waves, Fourier transforms, a max voltage of 220V and a min voltage of 0V? ↩︎ Interestingly, the Kerala government attempted to keep the usage of inverters in check. I have anecdotally heard that this is still an ongoing conversation / process. ↩︎ ","date":"2014-08-07","objectID":"/drafts/2014-08-07-whats-the-rationale-behind-power-cuts/:0:0","tags":["india","electricity","power","blackout"],"title":"What's the rationale behind power cuts?","uri":"/drafts/2014-08-07-whats-the-rationale-behind-power-cuts/"},{"categories":["programming"],"content":" My primary computing device for the last few years has been a 2009 netbook with an SU2300 dual core processor, and 2GB memory. I’ve managed to keep it up to date while being extremely performant and responsive by choosing the right set of tools and avoiding software bloat. My entire desktop boots up in less than 70MB of RAM and is extremely responsive. Here are the various components that make up my desktop: Operating System : Lubuntu: For a scaffolding installation, I use the latest LTS of Lubuntu, a lightweight version of Ubuntu1. This satisfies my requirements of stability, a large user community and regular updates. Window Manager : xmonad: While the LXDE is a fairly functional and responsive desktop environment, I use xmonad as my window manager. It’s a light weight, fast, tiling window manager written in Haskell. I am generally allergic to the mouse as an input device, and prefer to use the 70+ keys on my keyboard whenever possible. I’ve spent a good amount of time customising my window manager setup – it’s configured to run standalone. File Explorer : pcmanfm: If there ever was a fast, lightweight, X file system browser, this is it. It’s the default with LXDE and Lubuntu for a good reason. Besides being extremely fast to startup, it can run in daemon mode (making startups even faster), and manage desktop wallpaper. Network Manager : wicd: It’s been my choice of network managers for a while now, being able to handle any sort of Wifi network, auth, strength that I have had to deal with. I optionally use the wicd-curses UI via screen or the wicd-cli if I am connecting to a pre-configured network. Terminal Emulator : lxterminal: The default terminal emulator for LXDE, it is lightweight, and does a good job of staying out of one’s way – which is exactly what I require since I delegate most of my terminal management to screen (which also I customise). Shell : zsh: While bash is the unanimous choice for scripting, I don’t think anything beats the power of zsh for interactive sessions. Previously complex configuration has also become much easier with oh-my-zsh to manage themes and plugins. It completes everything from arguments to git branch names and even suggests corrections for typos! Again, I have customised my zsh configuration (this has gotten much smaller after oh-my-zsh). Volume Control: alsamixer: Good old alsamixer and it’s command line cousin amixer to the rescue. Nothing to see here, really, except more lightweight tools! Status Bar : dzen2: I use dzen 2 as my panel / status bar. It consists of two sections – the top left bar which xmonad manages (and pretty prints the current workspace and tiling modes), and the top right bar, which via a lightweight, completely non-portable2 hand written C-program displays memory, CPU, and battery statistics. Music Player : cmus, audacious: I switch betwee cmus, and audacious depending on whether I want to listen to a saved playlist (audacious), or my library (cmus). cmus is a console curses-based program, and does a very good job of library management, IMO. The latter is sort of a Winamp-clone on Linux. Browser : chrome, firefox, dwb : I switch heavily between firefox and chrome as I have found that firefox does better at managing 20+ tabs, and Chrome is more responsive in the fewer tabs scenarios. I use dwb, a lightweight, webkit based, browser that was inspired by the vimperator plugin, for most other light browsing (quick google search, etc..) as it has extremely good startup times for me - YMMV. With both chrome and firefox, I use vimium and vimperator respectively to keep my usage of the mouse to a minimum. So, there you have it – my light weight linux setup that’s fast, functional, and kind to my 5 year old netbook! Arch Linux would be a very good choice as well. ↩︎ It may work on other machines; I have not tested it. YMMV. ↩︎ ","date":"2014-07-31","objectID":"/posts/2014-07-31-my-linux-setup/:0:0","tags":["linux","software","xmonad"],"title":"My lightweight linux setup","uri":"/posts/2014-07-31-my-linux-setup/"},{"categories":["finance"],"content":" Ever since I read about the true cost of owning a car, I’ve generally thought about the true all-in costs of things – a phone plan, a housing loan, running a hypothetical chai shop, etc.. While I pursue this train of thought for purely intellectual purposes, it sometimes leads to conclusions that I find surprising. The most recent one for me was the true cost of renting an apartment in Bangalore – given the many (6-10) months deposit required, the post-lease maintenance clauses, and the prevalent brokerage charges. However, unlike the case of a car, there is no viable alternative to renting. Still, it makes for an interesting calculation. Anyhow, the point is the actual rent paid (and received by the owner in some form or the other) is very different from the “advertised rent” due to the above factors. Here’s a calculator I built to calculate the true cost of rent. I’ve set default values that I think are representative of what I have personally observed and heard about. In my current case for instance, the true cost of renting comes to 18% more than advertised: Monthly Rent Number of months rented Deposit Advance (in months) Painting charges in rupees Found via broker Calculate True Cost of Rent Summary: You are paying an extra Rs. (%) extra per month for a total true rent of Rs. per month. Over your tenure, the extra amount you are paying above your stated rent is Rs. : Assuming a risk free interest of 6% (say via fixed deposit) after taxes, you would have earnt Rs. over your tenure of months had you kept the deposit interest to your self. The re-painting charge of Rs. and a broker charge of Rs. added together comes to Rs. , your fixed costs of rent for your tenure. Adding the above two items together, the net cost is Rs.. Amortizing this over your stay of months, this results in paying Rs. extra per month. Note that this analysis assumes you have managed to obtain your original deposit amount back when you leave from your landlord and that you are not breaking the lease. Of course, given the sky high rental demand, the inherent power asymmetry, and the need for a roof over our heads, this is merely an intellectual exercise for most of us. ","date":"2014-07-24","objectID":"/drafts/2014-07-24-the-true-cost-of-rent/:0:0","tags":["personal-finance","rent","mortgage"],"title":"The true cost of renting","uri":"/drafts/2014-07-24-the-true-cost-of-rent/"},{"categories":["food"],"content":"Having lived in Bangalore for over three years now, I’ve heard a lot of people say that there just aren’t enough good, pocket friendly North Indian places to eat in Bangalore. I’ve experienced the contrary both because of where I live and my constant badgering of people for their recommendations. While I’m Madrasi (anywhere south of Mumbai, of course) and my culinary tasting abilities of the north are indeed questionable, I’ve heard nothing but good reviews of these places from others who frequent them as well. Here are the top 7 according to me. I’ve constrained myself here to a budget of Rs. 300 for two. Also do note that all theses places except the last two are not great in terms of ambience and are probably not places you would take a date to: Delhi Food Point: Located on stretch between Wipro Park, Koramangala and Jakkasandra, Delhi Food Point has some amazing food at very pocket friendly prices. I have been eating here for more than a year and a half now - the half-currys are a blessing for those who eat alone. Must tries here are the Tomato Fry, Daal Methi, and Bhindi Fry. A meal for two will probably cost you around Rs. 250. Ghar Ka Khana: You’ll have to look really hard to spot this place (they don’t even have a Zomato listing) - it’s located on the first floor of a building near the well known Cafe Thulp1. The veg thali and the “buffet” here are very good; the Daal Bhaati on Sundays is a must try. Their packaging for deliveries is extremely well done though they do take their time. I’m told they do have another branch or two in Bangalore. A meal for two will cost you around Rs. 200. Shree Shyam Rajasthani Thali: I found out about this place less than 3 months ago2 and have been a regular ever since. Located opposite Sree Krishna Kafe, their parathas and daal bhaatis are must haves. A meal for two here will probably set you back under Rs. 250. Sethji Ki Rasoi: My go-to place in Indiranagar (located on the road opposite Diamond District) when I’m craving for some pocket friendly North Indian food. Their chole and bhindi curries are very good (if a bit spicy, sometimes). A meal for two here should come to under Rs. 300. Paratha Plaza: Situated just behind Oasis Mall, Koramangala, this just made my list ahead of the competing Tattva Cafe. The sattu-based “Healthy Bihar” Paratha here is a must try. A meal for two here will probably total to well under Rs. 200. I believe they also have branches in HSR, JP Nagar, and BTM. Gud Dhaani: Located in BTM Layout, just beside the Lake Road, this place has both an extremely religious following and much better ambience than the places listed above. The “Hara Bhara Karela” here is absolutely mind blowing. It gets very crowded on Friday nights and weekends - expect a minimum 20 minute waiting time. A meal for two here will probably come to just over Rs. 300. Om Pure Veg: Although this place does stretch the above stated budget constraint, I simply cannot leave it out. While I have been a regular here for over two years now, the first time I ate here was only because I had been refused a table (an hour’s waiting) at the neighbouring Gramin! The thali here is definitely the centre piece; their Saboodana Kichdi is extremely appetizing as well. Add a very well done minimialist-themed ambience, some very good instrumental music, and a tagline that says “when you are tired of eating out”, it should be easy to understand why this place is a favourite of mine. A meal for two here will probably cost you just over Rs. Bonus: Poha at the Bikaner Sweets Centre (behind Wipro Park) for breakfast is some of the best I have ever had. It’s standing only and Poha finishes by 11am on most days. It’s popularity has tremendously increased over the last three years; be prepared to wait 15 minutes to get a plate to eat on weekend mornings: Hat tip, Arup, Nihar. ↩︎ Courtesy, Arpit. ↩︎ ","date":"2014-07-17","objectID":"/posts/2014-07-17-7-amazing-north-indian-restaurants-in-bangalore/:0:0","tags":["food","lists","india"],"title":"7 amazing budget friendly North Indian Places to eat in Bangalore","uri":"/posts/2014-07-17-7-amazing-north-indian-restaurants-in-bangalore/"},{"categories":null,"content":" I currently work at Google at the intersection of Gemini and Google Search. My experience includes prior work on Gemini and LaMDA, as well as Google Search’s Featured Snippets. Earlier in my career, I worked as a quantitative analyst on Wall Street. I hold Bachelor’s and Master’s degrees in Computer Science \u0026 Engineering from IIT Madras. ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Music My musical journey took an unexpected turn in college, where I began formal Carnatic music training. Though my initial steps were on the violin, guided by Ms. Sowmya (a student of Kanyakumari), I found my true passion in fretted instruments. Under the tutelage of Guru Shrinidhi Hemmige, I explored the intricacies of Carnatic guitar, fueled by the captivating sounds of Mandolin U Srinivas and Guitar R Prasanna, whose artistry continues to inspire me. I own several electric guitars and an electric 4-string mandolin. Keyboards Keyboards have held a special fascination for me, particularly as a heavy computer user. My journey began with a Kinesis Freestyle, mounted vertically to prioritize wrist comfort. This quickly grew into a genuine passion for collecting and using a variety of keyboards, even dabbling in custom builds. My daily driver is the Kinesis Advantage. Though I also own the newer Kinesis 360, finding myself still drawn to the classic Advantage 2. My collection include over a dozen split, ergonomic keyboards, including a custom Dactyl designed to perfectly fit my finger lengths, reflecting my dedication to finding the perfect typing experience. A few years ago, I decided to transition to the Colemak keyboard layout. The initial weeks and months were demanding, but the effort has paid off, as I am now comfortably “multi-layout fluent.” I maintain typing speeds well over 100 words per minute on both Colemak and the standard QWERTY layout. For a quick mental break, I often turn to typing games, finding them a fun and engaging way to sharpen my skills and refresh my mind. Coffee I enjoy a good cup of coffee, and I’ve even taken some beginner barista training to learn more about the process. My kitchen has a few different kinds of brewing equipment, and I tend to prefer lighter roasts from Central America when I drink it black. When I add milk, I like darker roasts from the Coorg or Chikmagalur regions of India. Perhaps this love is inherited; my great-grandmother ran a renowned coffee stop, opening her doors at 4:30 am to serve piping hot filter coffee to eager travelers. ","date":"0001-01-01","objectID":"/interests/:0:0","tags":null,"title":"Interests","uri":"/interests/"},{"categories":null,"content":"Google Scholar My publications are here. Software RL Snake - A snake playing agent trained using reinforcement learning - Demos in order of increasing complexity: Standard game (GIF) Standard game with obstacle (GIF) Vertical maze (GIF) Redhawk (Master’s Thesis) - A language agnostic abstract syntax tree based navigation system. Introductory Videos: Part 1: Introduction to the Redhawk navigation tool Part 2: Vim and Parallellism Part 3: More queries and the Selector Syntax Bristle - Making LaTeX Beamer fun again! Newsline - A fundamentally new causal way to look at news stories. Smile - A semantic analysis tool written from the ground up. MCMG - A Markov chain based Music Generator (for the Mohanam Raga). SimpleBot - A simple IRC bot. Cashman - Manage who owes who how much. Talks Introduction to Haskell (pdf) (resources) - Google Hyderabad Tech Talk, June 2010 Hack the what (pdf) - IIT Madras 2010 Hackfest Keynote Introduction to Python (pdf) - IIT Madras Linux Users Group 2010 The zen of Vim editing (pdf) - IIT Madras Linux Users Group 2010 Writings A slightly advanced introduction to vim - Linux Gazette 2008. Translated into French On programming interviews (pdf) Investing and financial independence for engineers (pdf) ","date":"0001-01-01","objectID":"/projects/:0:0","tags":["projects","hacks"],"title":"Projects","uri":"/projects/"}]